{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0dd861c-8b53-4b6f-90e8-e665ec5f95cc",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a63b0d3c-cf3d-4e8e-b4dd-27f7b8ef4eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30672d8-a385-4709-9e72-fdf546df51a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['This is a test sentence.', \"Let's see if it works!\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Devyani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Devyani/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSentences:\u001b[39m\u001b[33m\"\u001b[39m, sentences)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Then use word_tokenize as usual\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWords:\u001b[39m\u001b[33m\"\u001b[39m, word_tokenize(text))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = _get_punkt_tokenizer(language)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_lang(lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Devyani/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "# Explicitly load the correct Punkt model from the 'punkt' package\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Manually instantiate the tokenizer\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "\n",
    "# Use it directly if needed\n",
    "text = \"This is a test sentence. Let's see if it works!\"\n",
    "sentences = tokenizer.tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Then use word_tokenize as usual\n",
    "print(\"Words:\", word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e4301-73a7-495e-8b16-cd1dede3da05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5916764-235c-4b9b-9d58-05d7db5b9a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textstat import flesch_reading_ease\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82d2b2a9-35af-47d2-830f-498c84bf0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from textstat import flesch_reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f670128-c6f9-4b66-a56b-f894eb88cb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c0ec6-73d5-433a-915c-bab330af4fdc",
   "metadata": {},
   "source": [
    "### taking a closer look at cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7bfcb30-2f3f-422c-8424-c8f3f15cc89f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('coauthor_combined_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c285d819-f281-4838-b330-b73bfdb6230e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eventName</th>\n",
       "      <th>eventSource</th>\n",
       "      <th>eventTimestamp</th>\n",
       "      <th>textDelta</th>\n",
       "      <th>cursorRange</th>\n",
       "      <th>currentDoc</th>\n",
       "      <th>currentCursor</th>\n",
       "      <th>currentSuggestions</th>\n",
       "      <th>currentSuggestionIndex</th>\n",
       "      <th>currentHoverIndex</th>\n",
       "      <th>currentN</th>\n",
       "      <th>currentMaxToken</th>\n",
       "      <th>currentTemperature</th>\n",
       "      <th>currentTopP</th>\n",
       "      <th>currentPresencePenalty</th>\n",
       "      <th>currentFrequencyPenalty</th>\n",
       "      <th>eventNum</th>\n",
       "      <th>session_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>system-initialize</td>\n",
       "      <td>api</td>\n",
       "      <td>2021-08-16 07:00:41.033</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A woman has been dating guy after guy, but it ...</td>\n",
       "      <td>244</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>e0435f4cf6fc435c872ffc5b66b66b0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>text-insert</td>\n",
       "      <td>user</td>\n",
       "      <td>2021-08-16 07:00:46.487</td>\n",
       "      <td>{'ops': [{'retain': 244}, {'insert': ' '}]}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>245</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>e0435f4cf6fc435c872ffc5b66b66b0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>text-insert</td>\n",
       "      <td>user</td>\n",
       "      <td>2021-08-16 07:00:46.731</td>\n",
       "      <td>{'ops': [{'retain': 245}, {'insert': ' '}]}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>246</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>e0435f4cf6fc435c872ffc5b66b66b0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>text-insert</td>\n",
       "      <td>user</td>\n",
       "      <td>2021-08-16 07:00:46.897</td>\n",
       "      <td>{'ops': [{'retain': 246}, {'insert': ' '}]}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>247</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>e0435f4cf6fc435c872ffc5b66b66b0c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>text-delete</td>\n",
       "      <td>user</td>\n",
       "      <td>2021-08-16 07:00:47.247</td>\n",
       "      <td>{'ops': [{'retain': 246}, {'delete': 1}]}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>246</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>e0435f4cf6fc435c872ffc5b66b66b0c</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           eventName eventSource           eventTimestamp  \\\n",
       "0  system-initialize         api  2021-08-16 07:00:41.033   \n",
       "1        text-insert        user  2021-08-16 07:00:46.487   \n",
       "2        text-insert        user  2021-08-16 07:00:46.731   \n",
       "3        text-insert        user  2021-08-16 07:00:46.897   \n",
       "4        text-delete        user  2021-08-16 07:00:47.247   \n",
       "\n",
       "                                     textDelta cursorRange  \\\n",
       "0                                          NaN         NaN   \n",
       "1  {'ops': [{'retain': 244}, {'insert': ' '}]}         NaN   \n",
       "2  {'ops': [{'retain': 245}, {'insert': ' '}]}         NaN   \n",
       "3  {'ops': [{'retain': 246}, {'insert': ' '}]}         NaN   \n",
       "4    {'ops': [{'retain': 246}, {'delete': 1}]}         NaN   \n",
       "\n",
       "                                          currentDoc  currentCursor  \\\n",
       "0  A woman has been dating guy after guy, but it ...            244   \n",
       "1                                                NaN            245   \n",
       "2                                                NaN            246   \n",
       "3                                                NaN            247   \n",
       "4                                                NaN            246   \n",
       "\n",
       "  currentSuggestions  currentSuggestionIndex  currentHoverIndex  currentN  \\\n",
       "0                 []                       0                NaN         5   \n",
       "1                 []                       0                NaN         5   \n",
       "2                 []                       0                NaN         5   \n",
       "3                 []                       0                NaN         5   \n",
       "4                 []                       0                NaN         5   \n",
       "\n",
       "   currentMaxToken  currentTemperature  currentTopP  currentPresencePenalty  \\\n",
       "0               30                 0.3            1                       0   \n",
       "1               30                 0.3            1                       0   \n",
       "2               30                 0.3            1                       0   \n",
       "3               30                 0.3            1                       0   \n",
       "4               30                 0.3            1                       0   \n",
       "\n",
       "   currentFrequencyPenalty  eventNum                        session_id  \n",
       "0                      0.0         0  e0435f4cf6fc435c872ffc5b66b66b0c  \n",
       "1                      0.0         1  e0435f4cf6fc435c872ffc5b66b66b0c  \n",
       "2                      0.0         2  e0435f4cf6fc435c872ffc5b66b66b0c  \n",
       "3                      0.0         3  e0435f4cf6fc435c872ffc5b66b66b0c  \n",
       "4                      0.0         4  e0435f4cf6fc435c872ffc5b66b66b0c  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d520700-ba85-4f22-ab48-9cf7c79690ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>currentCursor</th>\n",
       "      <th>currentSuggestionIndex</th>\n",
       "      <th>currentHoverIndex</th>\n",
       "      <th>currentN</th>\n",
       "      <th>currentMaxToken</th>\n",
       "      <th>currentTemperature</th>\n",
       "      <th>currentTopP</th>\n",
       "      <th>currentPresencePenalty</th>\n",
       "      <th>currentFrequencyPenalty</th>\n",
       "      <th>eventNum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.701458e+06</td>\n",
       "      <td>2.701458e+06</td>\n",
       "      <td>2.299216e+06</td>\n",
       "      <td>2701458.0</td>\n",
       "      <td>2701458.0</td>\n",
       "      <td>2.701458e+06</td>\n",
       "      <td>2701458.0</td>\n",
       "      <td>2701458.0</td>\n",
       "      <td>2.701458e+06</td>\n",
       "      <td>2.701458e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.413145e+03</td>\n",
       "      <td>7.657195e-01</td>\n",
       "      <td>1.691181e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>5.214587e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.902231e-01</td>\n",
       "      <td>1.065809e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.792813e+02</td>\n",
       "      <td>1.238773e+00</td>\n",
       "      <td>1.432100e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.810858e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.878369e-01</td>\n",
       "      <td>7.630140e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2.000000e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8.050000e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.660000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.314000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>3.000000e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.000000e-01</td>\n",
       "      <td>9.440000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.880000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>7.500000e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.512000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.679000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.000000e-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>6.621000e+03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       currentCursor  currentSuggestionIndex  currentHoverIndex   currentN  \\\n",
       "count   2.701458e+06            2.701458e+06       2.299216e+06  2701458.0   \n",
       "mean    1.413145e+03            7.657195e-01       1.691181e+00        5.0   \n",
       "std     7.792813e+02            1.238773e+00       1.432100e+00        0.0   \n",
       "min     0.000000e+00            0.000000e+00       0.000000e+00        5.0   \n",
       "25%     8.050000e+02            0.000000e+00       0.000000e+00        5.0   \n",
       "50%     1.314000e+03            0.000000e+00       2.000000e+00        5.0   \n",
       "75%     1.880000e+03            1.000000e+00       3.000000e+00        5.0   \n",
       "max     5.679000e+03            4.000000e+00       4.000000e+00        5.0   \n",
       "\n",
       "       currentMaxToken  currentTemperature  currentTopP  \\\n",
       "count        2701458.0        2.701458e+06    2701458.0   \n",
       "mean              30.0        5.214587e-01          1.0   \n",
       "std                0.0        2.810858e-01          0.0   \n",
       "min               30.0        2.000000e-01          1.0   \n",
       "25%               30.0        3.000000e-01          1.0   \n",
       "50%               30.0        3.000000e-01          1.0   \n",
       "75%               30.0        7.500000e-01          1.0   \n",
       "max               30.0        9.000000e-01          1.0   \n",
       "\n",
       "       currentPresencePenalty  currentFrequencyPenalty      eventNum  \n",
       "count               2701458.0             2.701458e+06  2.701458e+06  \n",
       "mean                      0.0             4.902231e-01  1.065809e+03  \n",
       "std                       0.0             3.878369e-01  7.630140e+02  \n",
       "min                       0.0             0.000000e+00  0.000000e+00  \n",
       "25%                       0.0             0.000000e+00  4.660000e+02  \n",
       "50%                       0.0             5.000000e-01  9.440000e+02  \n",
       "75%                       0.0             1.000000e+00  1.512000e+03  \n",
       "max                       0.0             1.000000e+00  6.621000e+03  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef52914b-a57c-4a9f-ae20-2d996868979b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eventName', 'eventSource', 'eventTimestamp', 'textDelta', 'cursorRange', 'currentDoc', 'currentCursor', 'currentSuggestions', 'currentSuggestionIndex', 'currentHoverIndex', 'currentN', 'currentMaxToken', 'currentTemperature', 'currentTopP', 'currentPresencePenalty', 'currentFrequencyPenalty', 'eventNum', 'session_id']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0d29d1-d8fb-4e44-b31e-97435f12401c",
   "metadata": {},
   "source": [
    "### filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3951ce3a-b299-4380-b348-89133d3c9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[\n",
    "    (df['eventName'] == 'text-insert') &\n",
    "    (df['eventSource'].isin(['user', 'model'])) &\n",
    "    (df['textDelta'].notna()) &\n",
    "    (df['textDelta'].str.strip() != \"\")\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2629ad22-96fc-43ba-8c69-d025a9aaa18e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eventName', 'eventSource', 'eventTimestamp', 'textDelta', 'cursorRange', 'currentDoc', 'currentCursor', 'currentSuggestions', 'currentSuggestionIndex', 'currentHoverIndex', 'currentN', 'currentMaxToken', 'currentTemperature', 'currentTopP', 'currentPresencePenalty', 'currentFrequencyPenalty', 'eventNum', 'session_id']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcb86b1-8cc6-404a-aaed-df01d207fc92",
   "metadata": {},
   "source": [
    "### analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8800eb6-33cb-4b3c-b886-ad7e9cd529ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stylometric_features(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    words = [word for word in tokens if word.isalpha()]\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    return {\n",
    "        'readability': flesch_reading_ease(text),\n",
    "        'avg_word_length': np.mean([len(w) for w in words]) if words else 0,\n",
    "        'lexical_diversity': len(set(words)) / len(words) if words else 0,\n",
    "        'punctuation_freq': sum(1 for c in text if c in string.punctuation) / len(text) if text else 0,\n",
    "        'stopword_ratio': sum(1 for w in words if w.lower() in stop_words) / len(words) if words else 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7e791a2-d717-4e4d-ad06-863984a12fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Devyani/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c824dd13-c263-4189-9c89-c5954f9c3a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('/Users/Devyani/nltk_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9ceefc4-20f5-4fe8-9185-38d6b7841c0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Devyani/nltk_data'\n    - '/Applications/anaconda3/envs/mina/nltk_data'\n    - '/Applications/anaconda3/envs/mina/share/nltk_data'\n    - '/Applications/anaconda3/envs/mina/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Devyani/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m text = \u001b[38;5;28mstr\u001b[39m(row[\u001b[33m'\u001b[39m\u001b[33mtextDelta\u001b[39m\u001b[33m'\u001b[39m]) \n\u001b[32m      4\u001b[39m source = row[\u001b[33m'\u001b[39m\u001b[33meventSource\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m features = compute_stylometric_features(text)\n\u001b[32m      6\u001b[39m features[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m] = source\n\u001b[32m      7\u001b[39m features_list.append(features)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mcompute_stylometric_features\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_stylometric_features\u001b[39m(text):\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     tokens = word_tokenize(text)\n\u001b[32m      3\u001b[39m     words = [word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m word.isalpha()]\n\u001b[32m      4\u001b[39m     stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = _get_punkt_tokenizer(language)\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m PunktTokenizer(language)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28mself\u001b[39m.load_lang(lang)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = find(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtokenizers/punkt_tab/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlang\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Applications/anaconda3/envs/mina/lib/python3.13/site-packages/nltk/data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/Users/Devyani/nltk_data'\n    - '/Applications/anaconda3/envs/mina/nltk_data'\n    - '/Applications/anaconda3/envs/mina/share/nltk_data'\n    - '/Applications/anaconda3/envs/mina/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/Devyani/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "features_list = []\n",
    "for _, row in df.iterrows():\n",
    "    text = str(row['textDelta']) \n",
    "    source = row['eventSource']\n",
    "    features = compute_stylometric_features(text)\n",
    "    features['source'] = source\n",
    "    features_list.append(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24d7574-73ad-4425-a0a2-6e94411fc579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mina)",
   "language": "python",
   "name": "mina"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
